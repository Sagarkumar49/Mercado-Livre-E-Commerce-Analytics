{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43fad7b-9fd9-46eb-889d-fcdf6525b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "name=[]\n",
    "rating=[]\n",
    "review=[]\n",
    "url = \"https://www.flipkart.com/apple-iphone-15-black-128-gb/product-reviews/itm6ac6485515ae4?pid=MOBGTAGPTB3VS24W&lid=LSTMOBGTAGPTB3VS24WCTBCFM&marketplace=FLIPKART\"\n",
    "for i in range(2,6):\n",
    "    new_url= url+\"&page\"+str(i)\n",
    "    r=requests.get(new_url)\n",
    "    soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "    \n",
    "    Name= soup.find_all(\"p\",{\"class\":\"_2NsDsF AwS1CA\"})\n",
    "    for i in Name:\n",
    "        name.append(i.text)\n",
    "    name\n",
    "    Rating = soup.find_all(\"div\",{\"class\":\"XQDdHH Ga3i8K\"})\n",
    "    for i in Rating:\n",
    "        rating.append(i.text)\n",
    "    rating\n",
    "    Review = soup.find_all(\"div\",{\"class\":\"ZmyHeo\"})\n",
    "    for i in Review:\n",
    "        review.append(i.text)\n",
    "    review\n",
    "print(len(name),len(rating),len(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a05618-1c1c-4e37-9f5d-3d9aabd9ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf452ba6-cad6-4541-8574-b918e86944da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,6):\n",
    "    np=soup.find(\"a\",{\"class\":\"_9QVEpD\"}).get(\"href\")\n",
    "    cnp=\"https://www.flipkart.com\"+np[:-1]+str(i)\n",
    "    print(cnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e7a7822-9664-4917-a67c-378bcc6f3226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Found 0 reviews on this page.\n",
      "No more pages to scrape.\n",
      "Empty DataFrame\n",
      "Columns: [Username, Rating, Review]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries before running the script\n",
    "# Run the following command in your terminal or Jupyter Notebook:\n",
    "# !pip install selenium pandas webdriver-manager\n",
    "\n",
    "from selenium import webdriver  # Import Selenium WebDriver for browser automation\n",
    "from selenium.webdriver.chrome.service import Service  # Import Service for WebDriver management\n",
    "from selenium.webdriver.common.by import By  # Import By to locate elements\n",
    "from selenium.webdriver.chrome.options import Options  # Import Options to configure WebDriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager  # Import ChromeDriverManager for automatic driver management\n",
    "import time  # Import time for handling delays\n",
    "import pandas as pd  # Import pandas to store data in a structured format\n",
    "\n",
    "def scrape_flipkart_reviews(url, num_pages=5):\n",
    "    \"\"\"\n",
    "    Scrapes customer reviews from Flipkart product page using Selenium.\n",
    "    - Extracts username, rating, and review text.\n",
    "    - Handles pagination to scrape multiple pages.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode to avoid opening a browser window\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "    chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Overcome limited resources issues\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())  # Automatically download and set up ChromeDriver\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)  # Initialize WebDriver\n",
    "    \n",
    "    names, ratings, reviews = [], [], []  # Lists to store extracted data\n",
    "    \n",
    "    driver.get(url)  # Open the Flipkart review page\n",
    "    time.sleep(5)  # Wait for page to load\n",
    "    \n",
    "    for page in range(1, num_pages + 1):  # Loop through the required number of pages\n",
    "        print(f\"Scraping page {page}...\")  # Print progress\n",
    "        \n",
    "        review_containers = driver.find_elements(By.XPATH, \"//div[@class='_27M-vq']\")  # Locate review containers\n",
    "        print(f\"Found {len(review_containers)} reviews on this page.\")\n",
    "        \n",
    "        for review in review_containers:\n",
    "            try:\n",
    "                username = review.find_element(By.XPATH, \".//p[@class='_2sc7ZR _2V5EHH']\").text  # Extract the username\n",
    "                rating = review.find_element(By.XPATH, \".//div[@class='_3LWZlK _1BLPMq']\").text  # Extract the rating\n",
    "                review_text = review.find_element(By.XPATH, \".//div[@class='t-ZTKy']\").text  # Extract the review text\n",
    "                \n",
    "                names.append(username)\n",
    "                ratings.append(int(rating))\n",
    "                reviews.append(review_text.strip())\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping a review due to error: {e}\")  # Print error for debugging\n",
    "                continue  # Skip reviews with missing data\n",
    "        \n",
    "        # Pagination handling to move to the next page\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//a[contains(@class, '_1LKTO3') and contains(text(),'Next')]\" )  # Locate next page button\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)  # Click to move to next page\n",
    "            time.sleep(5)  # Wait for the next page to load\n",
    "        except Exception:\n",
    "            print(\"No more pages to scrape.\")  # Print message and break the loop\n",
    "            break  # Stop scraping further pages\n",
    "    \n",
    "    driver.quit()  # Close the WebDriver session\n",
    "    \n",
    "    return pd.DataFrame({'Username': names, 'Rating': ratings, 'Review': reviews})  # Return data as a DataFrame\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute scraping only.\n",
    "    \"\"\"\n",
    "    url = \"https://www.flipkart.com/apple-iphone-15-black-128-gb/product-reviews/itm6ac6485515ae4?pid=MOBGTAGPTB3VS24W&lid=LSTMOBGTAGPTB3VS24WCTBCFM&marketplace=FLIPKART\"  # Updated Flipkart product review URL\n",
    "    df_reviews = scrape_flipkart_reviews(url)  # Call the scraping function\n",
    "    \n",
    "    print(df_reviews.head())  # Print first few rows of the DataFrame for verification\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Execute the main function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd40bcdd-43fe-4257-bb8d-7dc0714db449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Found 0 reviews on this page.\n",
      "No more pages to scrape.\n",
      "Empty DataFrame\n",
      "Columns: [Username, Rating, Review]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries before running the script\n",
    "# Run the following command in your terminal or Jupyter Notebook:\n",
    "# !pip install selenium pandas webdriver-manager\n",
    "\n",
    "from selenium import webdriver  # Import Selenium WebDriver for browser automation\n",
    "from selenium.webdriver.chrome.service import Service  # Import Service for WebDriver management\n",
    "from selenium.webdriver.common.by import By  # Import By to locate elements\n",
    "from selenium.webdriver.chrome.options import Options  # Import Options to configure WebDriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager  # Import ChromeDriverManager for automatic driver management\n",
    "import time  # Import time for handling delays\n",
    "import pandas as pd  # Import pandas to store data in a structured format\n",
    "\n",
    "def scrape_flipkart_reviews(url, num_pages=5):\n",
    "    \"\"\"\n",
    "    Scrapes customer reviews from Flipkart product page using Selenium.\n",
    "    - Extracts username, rating, and review text.\n",
    "    - Handles pagination to scrape multiple pages.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode to avoid opening a browser window\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "    chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Overcome limited resources issues\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())  # Automatically download and set up ChromeDriver\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)  # Initialize WebDriver\n",
    "    \n",
    "    names, ratings, reviews = [], [], []  # Lists to store extracted data\n",
    "    \n",
    "    driver.get(url)  # Open the Flipkart review page\n",
    "    time.sleep(5)  # Wait for page to load\n",
    "    \n",
    "    for page in range(1, num_pages + 1):  # Loop through the required number of pages\n",
    "        print(f\"Scraping page {page}...\")  # Print progress\n",
    "        \n",
    "        review_containers = driver.find_elements(By.XPATH, \"//div[contains(@class, '_1AtVbE')]//div[@class='col _2wzgFH']\")  # Locate review containers\n",
    "        print(f\"Found {len(review_containers)} reviews on this page.\")\n",
    "        \n",
    "        for review in review_containers:\n",
    "            try:\n",
    "                username = review.find_element(By.XPATH, \"/html/body/div[1]/div/div[3]/div/div/div[2]/div[3]/div/div/div/div[4]/div[1]/p[1]\" ).text  # Extract the username\n",
    "                rating = review.find_element(By.XPATH, \"/html/body/div[1]/div/div[3]/div/div/div[2]/div[3]/div/div/div/div[1]/div\" ).text  # Extract the rating\n",
    "                review_text = review.find_element(By.XPATH, \"/html/body/div[1]/div/div[3]/div/div/div[2]/div[3]/div/div/div/div[1]/p\" ).text  # Extract the review text\n",
    "                \n",
    "                names.append(username)\n",
    "                ratings.append(int(rating))\n",
    "                reviews.append(review_text.strip())\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping a review due to error: {e}\")  # Print error for debugging\n",
    "                continue  # Skip reviews with missing data\n",
    "        \n",
    "        # Pagination handling to move to the next page\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//a[contains(@class, '_1LKTO3') and contains(text(),'Next')]\" )  # Locate next page button\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)  # Click to move to next page\n",
    "            time.sleep(5)  # Wait for the next page to load\n",
    "        except Exception:\n",
    "            print(\"No more pages to scrape.\")  # Print message and break the loop\n",
    "            break  # Stop scraping further pages\n",
    "    \n",
    "    driver.quit()  # Close the WebDriver session\n",
    "    \n",
    "    return pd.DataFrame({'Username': names, 'Rating': ratings, 'Review': reviews})  # Return data as a DataFrame\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute scraping only.\n",
    "    \"\"\"\n",
    "    url = \"https://www.flipkart.com/apple-iphone-15-black-128-gb/product-reviews/itm6ac6485515ae4?pid=MOBGTAGPTB3VS24W&lid=LSTMOBGTAGPTB3VS24WCTBCFM&marketplace=FLIPKART\"  # Updated Flipkart product review URL\n",
    "    df_reviews = scrape_flipkart_reviews(url)  # Call the scraping function\n",
    "    \n",
    "    print(df_reviews.head())  # Print first few rows of the DataFrame for verification\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Execute the main function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e4c2b1e-abe4-4c61-a7f8-c260fbebc402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page loaded successfully.\n",
      "Scraping page 1...\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "Message: \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df_reviews\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 81\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mMain function to execute scraping.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.flipkart.com/apple-iphone-15-black-128-gb/product-reviews/itm6ac6485515ae4?pid=MOBGTAGPTB3VS24W&lid=LSTMOBGTAGPTB3VS24WCTBCFM&marketplace=FLIPKART\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 81\u001b[0m df_reviews \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_flipkart_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_reviews\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[20], line 41\u001b[0m, in \u001b[0;36mscrape_flipkart_reviews\u001b[1;34m(url, num_pages)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Wait for review containers to be present\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m review_containers \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_all_elements_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m//div[contains(@class, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_1AtVbE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)]//div[@class=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcol _2wzgFH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(review_containers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reviews on this page.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m review_containers:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:146\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \n"
     ]
    }
   ],
   "source": [
    "# Install required libraries before running the script\n",
    "# Run in terminal or Jupyter: !pip install selenium pandas webdriver-manager\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_flipkart_reviews(url, num_pages=5):\n",
    "    \"\"\"\n",
    "    Scrapes customer reviews from Flipkart product page using Selenium.\n",
    "    - Extracts username, rating, and review text.\n",
    "    - Handles pagination to scrape multiple pages.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    wait = WebDriverWait(driver, 10)  # Wait up to 10 seconds for elements\n",
    "    \n",
    "    names, ratings, reviews = [], [], []\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        print(\"Page loaded successfully.\")\n",
    "        time.sleep(2)  # Initial delay for page load\n",
    "        \n",
    "        for page in range(1, num_pages + 1):\n",
    "            print(f\"Scraping page {page}...\")\n",
    "            \n",
    "            # Wait for review containers to be present\n",
    "            review_containers = wait.until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, \"//div[contains(@class, '_1AtVbE')]//div[@class='col _2wzgFH']\"))\n",
    "            )\n",
    "            print(f\"Found {len(review_containers)} reviews on this page.\")\n",
    "            \n",
    "            for review in review_containers:\n",
    "                try:\n",
    "                    username = review.find_element(By.XPATH, \".//p[contains(@class, '_2sc7ZR')]\").text\n",
    "                    rating = review.find_element(By.XPATH, \".//div[contains(@class, '_3LWZlK')]\").text\n",
    "                    review_text = review.find_element(By.XPATH, \".//div[@class='t-ZTKy']/div\").text\n",
    "                    \n",
    "                    names.append(username)\n",
    "                    ratings.append(float(rating))  # Use float to handle potential decimals\n",
    "                    reviews.append(review_text.strip())\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting review: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Pagination\n",
    "            if page < num_pages:\n",
    "                try:\n",
    "                    next_button = wait.until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, \"//a[contains(@class, '_1LKTO3') and contains(text(),'Next')]\"))\n",
    "                    )\n",
    "                    driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                    time.sleep(2)  # Short delay after clicking\n",
    "                except Exception as e:\n",
    "                    print(f\"No more pages available or error clicking Next: {e}\")\n",
    "                    break\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()  # Ensure driver closes even if an error occurs\n",
    "    \n",
    "    return pd.DataFrame({'Username': names, 'Rating': ratings, 'Review': reviews})\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute scraping.\n",
    "    \"\"\"\n",
    "    url = \"https://www.flipkart.com/apple-iphone-15-black-128-gb/product-reviews/itm6ac6485515ae4?pid=MOBGTAGPTB3VS24W&lid=LSTMOBGTAGPTB3VS24WCTBCFM&marketplace=FLIPKART\"\n",
    "    df_reviews = scrape_flipkart_reviews(url)\n",
    "    print(df_reviews.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64397cca-67f3-46fe-b24c-3940c5cb7b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "No reviews found on this page.\n",
      "Empty DataFrame\n",
      "Columns: [Username, Rating, Review]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries before running the script\n",
    "# Run the following command in your terminal or Jupyter Notebook:\n",
    "# !pip install selenium pandas webdriver-manager\n",
    "\n",
    "from selenium import webdriver  # Import Selenium WebDriver for browser automation\n",
    "from selenium.webdriver.chrome.service import Service  # Import Service for WebDriver management\n",
    "from selenium.webdriver.common.by import By  # Import By to locate elements\n",
    "from selenium.webdriver.chrome.options import Options  # Import Options to configure WebDriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # Import WebDriverWait for explicit waits\n",
    "from selenium.webdriver.support import expected_conditions as EC  # Import EC for wait conditions\n",
    "from webdriver_manager.chrome import ChromeDriverManager  # Import ChromeDriverManager for automatic driver management\n",
    "import time  # Import time for handling delays\n",
    "import pandas as pd  # Import pandas to store data in a structured format\n",
    "import random  # Import random for human-like delays\n",
    "\n",
    "def scrape_flipkart_reviews(url, num_pages=5):\n",
    "    \"\"\"\n",
    "    Scrapes customer reviews from Flipkart product page using Selenium.\n",
    "    - Extracts username, rating, and review text.\n",
    "    - Handles pagination to scrape multiple pages.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode to avoid opening a browser window\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "    chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Overcome limited resources issues\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())  # Automatically download and set up ChromeDriver\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)  # Initialize WebDriver\n",
    "    wait = WebDriverWait(driver, 10)  # Define explicit wait\n",
    "    \n",
    "    names, ratings, reviews = [], [], []  # Lists to store extracted data\n",
    "    \n",
    "    driver.get(url)  # Open the Flipkart review page\n",
    "    time.sleep(random.uniform(3, 6))  # Wait for page to load with random delay\n",
    "    \n",
    "    for page in range(1, num_pages + 1):  # Loop through the required number of pages\n",
    "        print(f\"Scraping page {page}...\")  # Print progress\n",
    "        \n",
    "        try:\n",
    "            review_containers = wait.until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, \"//div[@class='_27M-vq']\"))  # Updated XPath for reviews\n",
    "            )\n",
    "        except Exception:\n",
    "            print(\"No reviews found on this page.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Found {len(review_containers)} reviews on this page.\")\n",
    "        \n",
    "        for review in review_containers:\n",
    "            try:\n",
    "                username = review.find_element(By.XPATH, \".//p[contains(@class, '_2sc7ZR')]\").text  # Extract the username\n",
    "                rating = review.find_element(By.XPATH, \".//div[@class='_3LWZlK']\").text  # Extract the rating\n",
    "                review_text = review.find_element(By.XPATH, \".//div[@class='t-ZTKy']/div\").text  # Extract the review text\n",
    "                \n",
    "                names.append(username)\n",
    "                ratings.append(float(rating))  # Convert rating to float\n",
    "                reviews.append(review_text.strip())\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping a review due to error: {e}\")  # Print error for debugging\n",
    "                continue  # Skip reviews with missing data\n",
    "        \n",
    "        # Pagination handling to move to the next page\n",
    "        try:\n",
    "            next_button = wait.until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//a[contains(@class, '_1LKTO3') and contains(text(),'Next')]\")\n",
    "            ))  # Locate next page button\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)  # Click to move to next page\n",
    "            time.sleep(random.uniform(3, 6))  # Random delay for human-like behavior\n",
    "        except Exception:\n",
    "            print(\"No more pages to scrape.\")  # Print message and break the loop\n",
    "            break  # Stop scraping further pages\n",
    "    \n",
    "    driver.quit()  # Close the WebDriver session\n",
    "    \n",
    "    return pd.DataFrame({'Username': names, 'Rating': ratings, 'Review': reviews})  # Return data as a DataFrame\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute scraping only.\n",
    "    \"\"\"\n",
    "    url = \"https://www.flipkart.com/apple-iphone-15-black-128-gb/product-reviews/itm6ac6485515ae4?pid=MOBGTAGPTB3VS24W&lid=LSTMOBGTAGPTB3VS24WCTBCFM&marketplace=FLIPKART\"  # Updated Flipkart product review URL\n",
    "    df_reviews = scrape_flipkart_reviews(url)  # Call the scraping function\n",
    "    \n",
    "    print(df_reviews.head())  # Print first few rows of the DataFrame for verification\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Execute the main function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab060c-f7ea-48f8-ab55-a80f974406d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying URL: https://www.flipkart.com/apple-iphone-15-black-128-gb/p/itm6ac6485515ae4\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def scrape_flipkart_reviews(url, num_pages=10):\n",
    "    chrome_options = Options()\n",
    "    # Remove headless mode to see what's happening (for debugging)\n",
    "    # chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    # Add a user-agent to avoid being detected as a bot\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\")\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    \n",
    "    names, ratings, reviews = [], [], []\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Give page more time to load\n",
    "        \n",
    "        # Check if we're on a review page\n",
    "        print(f\"Current URL: {driver.current_url}\")\n",
    "        \n",
    "        # Try to find the \"All Reviews\" button if we're on the product page\n",
    "        try:\n",
    "            all_reviews_button = driver.find_element(By.XPATH, \"//div[contains(text(), 'All Reviews')]\")\n",
    "            driver.execute_script(\"arguments[0].click();\", all_reviews_button)\n",
    "            time.sleep(3)\n",
    "        except Exception as e:\n",
    "            print(f\"No 'All Reviews' button found: {e}\")\n",
    "        \n",
    "        for page in range(1, num_pages + 1):\n",
    "            print(f\"Scraping page {page}...\")\n",
    "            \n",
    "            # Updated XPath selectors - try different variations\n",
    "            try:\n",
    "                # Try multiple selectors for review containers\n",
    "                selectors = [\n",
    "                    \"//div[contains(@class, '_27M-vq')]\",\n",
    "                    \"//div[contains(@class, '_1AtVbE')]//div[contains(@class, 'col')]\",\n",
    "                    \"//div[contains(@class, 'col _2wzgFH')]\",\n",
    "                    \"//div[@class='_1YokD2 _3Mn1Gg']/div[@class='_1AtVbE']\"\n",
    "                ]\n",
    "                \n",
    "                review_containers = None\n",
    "                for selector in selectors:\n",
    "                    try:\n",
    "                        review_containers = wait.until(\n",
    "                            EC.presence_of_all_elements_located((By.XPATH, selector))\n",
    "                        )\n",
    "                        if review_containers and len(review_containers) > 0:\n",
    "                            print(f\"Found {len(review_containers)} reviews using selector: {selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if not review_containers or len(review_containers) == 0:\n",
    "                    print(\"No reviews found on this page with any selector.\")\n",
    "                    # For debugging, save page source\n",
    "                    with open(f\"page_source_{page}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(driver.page_source)\n",
    "                    break\n",
    "                \n",
    "                for review in review_containers:\n",
    "                    try:\n",
    "                        # Try different XPath combinations for username\n",
    "                        username_selectors = [\n",
    "                            \".//p[contains(@class, '_2sc7ZR')]\",\n",
    "                            \".//p[@class='_2sc7ZR _2V5EHH']\",\n",
    "                            \".//div[contains(@class, '_3LWZlK')]/following-sibling::div//p\"\n",
    "                        ]\n",
    "                        \n",
    "                        username = None\n",
    "                        for selector in username_selectors:\n",
    "                            try:\n",
    "                                username_elem = review.find_element(By.XPATH, selector)\n",
    "                                if username_elem:\n",
    "                                    username = username_elem.text\n",
    "                                    break\n",
    "                            except:\n",
    "                                continue\n",
    "                        \n",
    "                        if not username:\n",
    "                            username = \"Anonymous\"\n",
    "                        \n",
    "                        # Try different XPath combinations for rating\n",
    "                        rating_selectors = [\n",
    "                            \".//div[@class='_3LWZlK']\",\n",
    "                            \".//div[contains(@class, '_3LWZlK')]\"\n",
    "                        ]\n",
    "                        \n",
    "                        rating = None\n",
    "                        for selector in rating_selectors:\n",
    "                            try:\n",
    "                                rating_elem = review.find_element(By.XPATH, selector)\n",
    "                                if rating_elem:\n",
    "                                    rating = rating_elem.text\n",
    "                                    break\n",
    "                            except:\n",
    "                                continue\n",
    "                        \n",
    "                        if not rating:\n",
    "                            continue  # Skip if no rating found\n",
    "                        \n",
    "                        # Try different XPath combinations for review text\n",
    "                        review_selectors = [\n",
    "                            \".//div[@class='t-ZTKy']/div\",\n",
    "                            \".//div[contains(@class, 't-ZTKy')]//div\",\n",
    "                            \".//div[contains(@class, 't-ZTKy')]//span\"\n",
    "                        ]\n",
    "                        \n",
    "                        review_text = None\n",
    "                        for selector in review_selectors:\n",
    "                            try:\n",
    "                                review_elem = review.find_element(By.XPATH, selector)\n",
    "                                if review_elem:\n",
    "                                    review_text = review_elem.text\n",
    "                                    break\n",
    "                            except:\n",
    "                                continue\n",
    "                        \n",
    "                        if not review_text:\n",
    "                            continue  # Skip if no review text found\n",
    "                        \n",
    "                        names.append(username)\n",
    "                        ratings.append(float(rating))\n",
    "                        reviews.append(review_text.strip())\n",
    "                        print(f\"Added review: {username[:10]}... - Rating: {rating}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing review: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Try to find next button (different ways)\n",
    "                next_button = None\n",
    "                next_selectors = [\n",
    "                    \"//a[contains(@class, '_1LKTO3') and contains(text(),'Next')]\",\n",
    "                    \"//span[text()='Next']/parent::a\",\n",
    "                    \"//a[contains(text(),'Next')]\"\n",
    "                ]\n",
    "                \n",
    "                for selector in next_selectors:\n",
    "                    try:\n",
    "                        next_button = driver.find_element(By.XPATH, selector)\n",
    "                        if next_button:\n",
    "                            print(f\"Found next button with selector: {selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if next_button:\n",
    "                    driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                    time.sleep(random.uniform(3, 6))\n",
    "                else:\n",
    "                    print(\"No more pages to scrape (next button not found).\")\n",
    "                    break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error on page {page}: {e}\")\n",
    "                break\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    # Create DataFrame with collected data\n",
    "    df = pd.DataFrame({'Username': names, 'Rating': ratings, 'Review': reviews})\n",
    "    print(f\"Total reviews collected: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Try both the product page URL and direct review URL\n",
    "    urls = [\n",
    "        \"https://www.flipkart.com/apple-iphone-15-black-128-gb/p/itm6ac6485515ae4\",\n",
    "        \"https://www.flipkart.com/apple-iphone-15-black-128-gb/product-reviews/itm6ac6485515ae4?pid=MOBGTAGPTB3VS24W&lid=LSTMOBGTAGPTB3VS24WCTBCFM&marketplace=FLIPKART\"\n",
    "    ]\n",
    "    \n",
    "    for url in urls:\n",
    "        print(f\"Trying URL: {url}\")\n",
    "        df_reviews = scrape_flipkart_reviews(url)\n",
    "        \n",
    "        if not df_reviews.empty:\n",
    "            print(\"Scraping successful!\")\n",
    "            print(df_reviews.head())\n",
    "            # Save to CSV for later use\n",
    "            df_reviews.to_csv('iphone15_reviews.csv', index=False)\n",
    "            break\n",
    "        else:\n",
    "            print(f\"No reviews found with URL: {url}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c5f696-3628-4bf1-9851-d1dcfef44f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping completed! Data saved to flipkart_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Base URL of Flipkart product reviews\n",
    "base_url = \"https://www.flipkart.com/apple-iphone-15-black-128-gb/product-reviews/itm6ac6485515ae4\"\n",
    "\n",
    "# Headers to mimic a real browser request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def get_reviews(page_num):\n",
    "    \"\"\"Scrape reviews from a given page number.\"\"\"\n",
    "    url = f\"{base_url}?page={page_num}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page_num}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    review_blocks = soup.find_all(\"div\", class_=\"_27M-vq\")  # Class for review block\n",
    "    \n",
    "    reviews = []\n",
    "    for block in review_blocks:\n",
    "        try:\n",
    "            username = block.find(\"p\", class_=\"_2sc7ZR _2V5EHH\").text.strip()\n",
    "            rating = block.find(\"div\", class_=\"_3LWZlK\").text.strip()\n",
    "            review_text = block.find(\"div\", class_=\"t-ZTKy\").text.strip()\n",
    "            reviews.append([username, rating, review_text])\n",
    "        except AttributeError:\n",
    "            continue  # Skip if any element is missing\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "# Collecting reviews from multiple pages\n",
    "total_reviews = []\n",
    "for page in range(1, 15):  # Adjust range to get at least 300 reviews\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    total_reviews.extend(get_reviews(page))\n",
    "    time.sleep(2)  # Adding delay to prevent getting blocked\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "reviews_df = pd.DataFrame(total_reviews, columns=[\"Username\", \"Rating\", \"Review Text\"])\n",
    "reviews_df.to_csv(\"flipkart_reviews.csv\", index=False)\n",
    "print(\"Scraping completed! Data saved to flipkart_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7fc56db-c48f-4ecf-b7bc-15d0b54adb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping completed! Here's a preview of the data:\n",
      "Empty DataFrame\n",
      "Columns: [Username, Rating, Review Text]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Base URL of Flipkart product reviews\n",
    "base_url = \"https://www.flipkart.com/apple-iphone-15-black-128-gb/product-reviews/itm6ac6485515ae4\"\n",
    "\n",
    "# Headers to mimic a real browser request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def get_reviews(page_num):\n",
    "    \"\"\"Scrape reviews from a given page number.\"\"\"\n",
    "    url = f\"{base_url}?page={page_num}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page_num}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    review_blocks = soup.find_all(\"div\", class_=\"_27M-vq\")  # Class for review block\n",
    "    \n",
    "    reviews = []\n",
    "    for block in review_blocks:\n",
    "        try:\n",
    "            username = block.find(\"p\", class_=\"_2sc7ZR _2V5EHH\").text.strip()\n",
    "            rating = block.find(\"div\", class_=\"_3LWZlK\").text.strip()\n",
    "            review_text = block.find(\"div\", class_=\"t-ZTKy\").text.strip()\n",
    "            reviews.append([username, rating, review_text])\n",
    "        except AttributeError:\n",
    "            continue  # Skip if any element is missing\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "# Collecting reviews from multiple pages\n",
    "total_reviews = []\n",
    "for page in range(1, 15):  # Adjust range to get at least 300 reviews\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    total_reviews.extend(get_reviews(page))\n",
    "    time.sleep(2)  # Adding delay to prevent getting blocked\n",
    "\n",
    "# Convert to DataFrame\n",
    "reviews_df = pd.DataFrame(total_reviews, columns=[\"Username\", \"Rating\", \"Review Text\"])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"Scraping completed! Here's a preview of the data:\")\n",
    "print(reviews_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33511e95-9df5-4402-bfcb-4cb18621ed70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Page 1 HTML preview:\n",
      " <!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <link href=\"https://rukminim2.flixcart.com\" rel=\"preconnect\"/>\n",
      "  <link href=\"//static-assets-web.flixcart.com/fk-p-linchpin-web/fk-cp-zion/css/atlas.chunk.8dd48d.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//static-assets-web.flixcart.com/fk-p-linchpin-web/fk-cp-zion/css/app_modules.chunk.c48a12.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//static-assets-web.flixcart.com/fk-p-linchpin-web/fk-cp-zion/css/app.chunk.e4e719.css\" rel=\"stylesheet\"/>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\"/>\n",
      "  <meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "  <meta content=\"102988293558\" property=\"fb:page_id\"/>\n",
      "  <meta content=\"658873552,624500995,100000233612389\" property=\"fb:admins\"/>\n",
      "  <link href=\"https://static-assets-web.flixcart.com/www/promos/new/20150528-140547-favicon-retina.ico\" rel=\"shortcut icon\"/>\n",
      "  <link href=\"/osdd.xml?v=2\" rel=\"search\" type=\"application/opensearchdescription+xml\"/>\n",
      "  <meta content=\"website\" property=\"og:type\"/>\n",
      "Scraping page 2...\n",
      "Page 2 HTML preview:\n",
      " <!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <link href=\"https://rukminim2.flixcart.com\" rel=\"preconnect\"/>\n",
      "  <link href=\"//static-assets-web.flixcart.com/fk-p-linchpin-web/fk-cp-zion/css/atlas.chunk.8dd48d.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//static-assets-web.flixcart.com/fk-p-linchpin-web/fk-cp-zion/css/app_modules.chunk.c48a12.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//static-assets-web.flixcart.com/fk-p-linchpin-web/fk-cp-zion/css/app.chunk.e4e719.css\" rel=\"stylesheet\"/>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\"/>\n",
      "  <meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "  <meta content=\"102988293558\" property=\"fb:page_id\"/>\n",
      "  <meta content=\"658873552,624500995,100000233612389\" property=\"fb:admins\"/>\n",
      "  <link href=\"https://static-assets-web.flixcart.com/www/promos/new/20150528-140547-favicon-retina.ico\" rel=\"shortcut icon\"/>\n",
      "  <link href=\"/osdd.xml?v=2\" rel=\"search\" type=\"application/opensearchdescription+xml\"/>\n",
      "  <meta content=\"website\" property=\"og:type\"/>\n",
      "Scraping page 3...\n",
      "Page 3 HTML preview:\n",
      " <!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <link href=\"https://rukminim2.flixcart.com\" rel=\"preconnect\"/>\n",
      "  <link href=\"//static-assets-web.flixcart.com/fk-p-linchpin-web/fk-cp-zion/css/atlas.chunk.8dd48d.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//static-assets-web.flixcart.com/fk-p-linchpin-web/fk-cp-zion/css/app_modules.chunk.c48a12.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//static-assets-web.flixcart.com/fk-p-linchpin-web/fk-cp-zion/css/app.chunk.e4e719.css\" rel=\"stylesheet\"/>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\"/>\n",
      "  <meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "  <meta content=\"102988293558\" property=\"fb:page_id\"/>\n",
      "  <meta content=\"658873552,624500995,100000233612389\" property=\"fb:admins\"/>\n",
      "  <link href=\"https://static-assets-web.flixcart.com/www/promos/new/20150528-140547-favicon-retina.ico\" rel=\"shortcut icon\"/>\n",
      "  <link href=\"/osdd.xml?v=2\" rel=\"search\" type=\"application/opensearchdescription+xml\"/>\n",
      "  <meta content=\"website\" property=\"og:type\"/>\n",
      "Scraping page 4...\n",
      "Page 4 HTML preview:\n",
      " <!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <link href=\"https://rukminim2.flixcart.com\" rel=\"preconnect\"/>\n",
      "  <link href=\"//static-assets-web.flixcart.com/fk-p-linchpin-web/fk-cp-zion/css/atlas.chunk.8dd48d.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//static-assets-web.flixcart.com/fk-p-linchpin-web/fk-cp-zion/css/app_modules.chunk.c48a12.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"//static-assets-web.flixcart.com/fk-p-linchpin-web/fk-cp-zion/css/app.chunk.e4e719.css\" rel=\"stylesheet\"/>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\"/>\n",
      "  <meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "  <meta content=\"102988293558\" property=\"fb:page_id\"/>\n",
      "  <meta content=\"658873552,624500995,100000233612389\" property=\"fb:admins\"/>\n",
      "  <link href=\"https://static-assets-web.flixcart.com/www/promos/new/20150528-140547-favicon-retina.ico\" rel=\"shortcut icon\"/>\n",
      "  <link href=\"/osdd.xml?v=2\" rel=\"search\" type=\"application/opensearchdescription+xml\"/>\n",
      "  <meta content=\"website\" property=\"og:type\"/>\n",
      "Scraping completed! Here's a preview of the data:\n",
      "Empty DataFrame\n",
      "Columns: [Username, Rating, Review Text]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Base URL of Flipkart product reviews\n",
    "base_url = \"https://www.flipkart.com/apple-iphone-15-black-128-gb/product-reviews/itm6ac6485515ae4\"\n",
    "\n",
    "# Headers to mimic a real browser request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def get_reviews(page_num):\n",
    "    \"\"\"Scrape reviews from a given page number.\"\"\"\n",
    "    url = f\"{base_url}?page={page_num}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page_num}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Debugging: Print a small part of the HTML to check if data is received\n",
    "    print(f\"Page {page_num} HTML preview:\\n\", soup.prettify()[:1000])\n",
    "    \n",
    "    review_blocks = soup.find_all(\"div\", class_=\"_1AtVbE\")  # Updated class for review block\n",
    "    \n",
    "    reviews = []\n",
    "    for block in review_blocks:\n",
    "        try:\n",
    "            username = block.find(\"p\", class_=\"_2sc7ZR\").text.strip()\n",
    "            rating = block.find(\"div\", class_=\"_3LWZlK\").text.strip()\n",
    "            review_text = block.find(\"div\", class_=\"t-ZTKy\").text.strip()\n",
    "            reviews.append([username, rating, review_text])\n",
    "        except AttributeError:\n",
    "            continue  # Skip if any element is missing\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "# Collecting reviews from multiple pages\n",
    "total_reviews = []\n",
    "for page in range(1, 5):  # Reduced to 5 pages for debugging\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    total_reviews.extend(get_reviews(page))\n",
    "    time.sleep(2)  # Adding delay to prevent getting blocked\n",
    "\n",
    "# Convert to DataFrame\n",
    "reviews_df = pd.DataFrame(total_reviews, columns=[\"Username\", \"Rating\", \"Review Text\"])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"Scraping completed! Here's a preview of the data:\")\n",
    "print(reviews_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce8d3de-b609-4c16-8a2c-6cd1c6f04281",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDriverException",
     "evalue": "Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:64\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe path is not a valid file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m path\n",
      "\u001b[1;31mValueError\u001b[0m: The path is not a valid file: chromedriver",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNoSuchDriverException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Path to ChromeDriver (Ensure you have it installed)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m service \u001b[38;5;241m=\u001b[39m Service(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchromedriver\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Base URL of Flipkart product reviews\u001b[39;00m\n\u001b[0;32m     21\u001b[0m base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.flipkart.com/apple-iphone-15-black-128-gb/product-reviews/itm6ac6485515ae4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:50\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[0;32m     49\u001b[0m finder \u001b[38;5;241m=\u001b[39m DriverFinder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice, options)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_browser_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     51\u001b[0m     options\u001b[38;5;241m.\u001b[39mbinary_location \u001b[38;5;241m=\u001b[39m finder\u001b[38;5;241m.\u001b[39mget_browser_path()\n\u001b[0;32m     52\u001b[0m     options\u001b[38;5;241m.\u001b[39mbrowser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:47\u001b[0m, in \u001b[0;36mDriverFinder.get_browser_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_browser_path\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_binary_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowser_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:78\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     77\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrowser\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths\n",
      "\u001b[1;31mNoSuchDriverException\u001b[0m: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up Selenium WebDriver options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode (no UI)\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Path to ChromeDriver (Ensure you have it installed)\n",
    "service = Service(\"chromedriver\")\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Base URL of Flipkart product reviews\n",
    "base_url = \"https://www.flipkart.com/apple-iphone-15-black-128-gb/product-reviews/itm6ac6485515ae4\"\n",
    "\n",
    "def get_reviews(page_num):\n",
    "    \"\"\"Scrape reviews using Selenium.\"\"\"\n",
    "    url = f\"{base_url}?page={page_num}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Wait for JavaScript to load content\n",
    "    \n",
    "    reviews = []\n",
    "    review_blocks = driver.find_elements(By.CLASS_NAME, \"_27M-vq\")  # Class for review container\n",
    "    \n",
    "    for block in review_blocks:\n",
    "        try:\n",
    "            username = block.find_element(By.CLASS_NAME, \"_2sc7ZR\").text.strip()\n",
    "            rating = block.find_element(By.CLASS_NAME, \"_3LWZlK\").text.strip()\n",
    "            review_text = block.find_element(By.CLASS_NAME, \"t-ZTKy\").text.strip()\n",
    "            reviews.append([username, rating, review_text])\n",
    "        except:\n",
    "            continue  # Skip if any element is missing\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "# Collecting reviews from multiple pages\n",
    "total_reviews = []\n",
    "for page in range(1, 5):  # Scraping 5 pages for debugging\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    total_reviews.extend(get_reviews(page))\n",
    "    time.sleep(2)\n",
    "\n",
    "driver.quit()  # Close the browser session\n",
    "\n",
    "# Convert to DataFrame\n",
    "reviews_df = pd.DataFrame(total_reviews, columns=[\"Username\", \"Rating\", \"Review Text\"])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"Scraping completed! Here's a preview of the data:\")\n",
    "print(reviews_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
